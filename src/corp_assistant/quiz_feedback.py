# src/corp_assistant/quiz_feedback.py
from langchain_openai import ChatOpenAI
from corp_assistant.rag import get_vectorstore_by_collection
import os

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0, streaming=True)
def is_running_in_studio() -> bool:
    try:
        _ = input("í…ŒìŠ¤íŠ¸ ì…ë ¥: ")  # Studioì—ì„  input í˜¸ì¶œ ì‹œ EOFError ë°œìƒ
    except EOFError:
        return True
    return False

def quiz_feedback_node(state: dict):
    is_studio = is_running_in_studio()
    print(f"ğŸ§ª ì‹¤í–‰ í™˜ê²½: {'LangGraph Studio ë‚´ë¶€' if is_studio else 'ë¡œì»¬'}")
    retriever = get_vectorstore_by_collection("quiz-corp").as_retriever()
    docs = retriever.invoke("ì˜¨ë³´ë”© í€´ì¦ˆ")
    questions = docs[0].page_content.strip()

    print("\nğŸ“ í€´ì¦ˆ:\n" + questions)

    if is_studio:
        input_data = state.get("input_data", {}) or state.get("input", {})
        user_answer = input_data.get("answer")
        if not user_answer:
            raise ValueError("ë‹µì•ˆì´ ì—†ìŠµë‹ˆë‹¤. LangGraph Studioì—ì„œëŠ” input.answerì— ê°’ì„ ë„£ì–´ì£¼ì„¸ìš”.",state)
    else:
        user_answer = input("\nâœï¸ ë‹µì•ˆì„ ì…ë ¥í•˜ì„¸ìš”:\n")

    safe_answer = user_answer.encode("utf-8", "ignore").decode("utf-8")

    prompt = f"""
    ì•„ë˜ëŠ” í€´ì¦ˆ ë¬¸ì œì™€ ì‚¬ìš©ìì˜ ë‹µë³€ì…ë‹ˆë‹¤. GPTë¡œì„œ ì ì ˆíˆ ì±„ì í•˜ê³  ìƒì„¸í•˜ê²Œ í”¼ë“œë°±ì„ ì£¼ì‹­ì‹œì˜¤.

    [ë¬¸ì œ]
    {questions}

    [ë‹µë³€]
    {safe_answer}

    [í”¼ë“œë°± ì–‘ì‹]
    ì ìˆ˜: xx/100
    ê°•ì : ...
    ë³´ì™„í•  ì : ...
    """

    print("\nğŸ“Š ì±„ì  ê²°ê³¼:")
    feedback_text = ""
    for chunk in llm.stream(prompt):
        print(chunk.content, end="", flush=True)
        feedback_text += chunk.content

    if not is_studio:
        print("\n\nğŸ“Œ ì„ íƒí•˜ì„¸ìš”:")
        print("1. ë¶€ì¡±í•œ ì  ìì„¸íˆ ì„¤ëª…í•´ì¤˜")
        print("2. ë‹¤ì‹œ í’€ì–´ë³¼ë˜ (ì¤€ë¹„ ì¤‘)")
        print("3. ê·¸ë§Œí•˜ê¸°")

        choice = input("ğŸ‘‰ ë²ˆí˜¸ ì…ë ¥: ").strip()

        if choice == "1":
            print("\nğŸ” ë¶€ì¡±í•œ ì  ìƒì„¸ ì„¤ëª…:")
            followup_prompt = f"""
            ì•„ë˜ëŠ” GPTê°€ ì±„ì í•œ í€´ì¦ˆ í”¼ë“œë°±ì…ë‹ˆë‹¤.
            'ë³´ì™„í•  ì ' í•­ëª©ì„ ë°”íƒ•ìœ¼ë¡œ, ì‹ ì… ê°œë°œìê°€ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì˜ˆì‹œì™€ í•¨ê»˜ ìƒì„¸í•˜ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.

            [í”¼ë“œë°± ì›ë¬¸]
            {feedback_text}
            """
            for chunk in llm.stream(followup_prompt):
                print(chunk.content, end="", flush=True)

        elif choice == "2":
            print("ğŸ” ë‹¤ì‹œ í’€ê¸° ê¸°ëŠ¥ì€ í˜„ì¬ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.")
        else:
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")

    return {"quiz_result": feedback_text}